import spacy
from logger import log_info, log_warning


# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é spaCy
nlp = spacy.load("ru_core_news_lg")

# –°–ª–æ–≤–∞—Ä—å —Å–∏–Ω–æ–Ω–∏–º–æ–≤ –¥–ª—è –∫–ª—é—á–µ–≤—ã—Ö —Ç–µ–º, –≤–∫–ª—é—á–∞—è –±–µ–≥ –∏ –≥–æ—Ä–Ω—ã–µ –≤–∏–¥—ã —Å–ø–æ—Ä—Ç–∞
synonyms = {
    "morning": ["—É—Ç—Ä–æ", "–∑–∞–≤—Ç—Ä–∞–∫", "–Ω–∞—á–∞–ª–æ –¥–Ω—è", "—É—Ç—Ä–µ–Ω–Ω–∏–π", "—É—Ç—Ä–µ–Ω–Ω–µ–µ –≤—Ä–µ–º—è"],
    "energy": ["—ç–Ω–µ—Ä–≥–∏—è", "–∂–∏–∑–Ω–µ–Ω–Ω—ã–µ —Å–∏–ª—ã", "–∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å", "—ç–Ω–µ—Ä–≥–∏—á–Ω—ã–π", "–≤–¥–æ—Ö–Ω–æ–≤–µ–Ω–∏–µ"],
    "motivation": ["–º–æ—Ç–∏–≤–∞—Ü–∏—è", "—Ü–µ–ª—å", "–Ω–∞—Å—Ç—Ä–æ–π", "—Å—Ç—Ä–µ–º–ª–µ–Ω–∏–µ", "–∂–µ–ª–∞–Ω–∏–µ"],
    "health": ["–∑–¥–æ—Ä–æ–≤—å–µ", "—Ñ–∏–∑–∏—á–µ—Å–∫–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ", "–±–ª–∞–≥–æ–ø–æ–ª—É—á–∏–µ", "—Ñ–∏–∑–∫—É–ª—å—Ç—É—Ä–∞", "–±–∏–æ—Ö–∞–∫–∏–Ω–≥"],
    "fitness": ["—Å–ø–æ—Ä—Ç", "—Ñ–∏—Ç–Ω–µ—Å", "—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏", "—É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è", "—Ñ–∏–∑–∏—á–µ—Å–∫–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å"],
    "productivity": ["–ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—å", "—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å", "—Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏–≤–Ω–æ—Å—Ç—å", "—Ü–µ–ª–∏", "–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ"],
    "habit": ["–ø—Ä–∏–≤—ã—á–∫–∞", "–µ–∂–µ–¥–Ω–µ–≤–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è", "—Ä—É—Ç–∏–Ω–∞", "—Ä–∏—Ç—É–∞–ª"],
    "nature": ["–ø—Ä–∏—Ä–æ–¥–∞", "–æ–∫—Ä—É–∂–∞—é—â–∞—è —Å—Ä–µ–¥–∞", "–ª–∞–Ω–¥—à–∞—Ñ—Ç", "–ø–µ–π–∑–∞–∂", "—ç–∫–æ—Å–∏—Å—Ç–µ–º–∞"],
    "running": ["–±–µ–≥", "–º–∞—Ä–∞—Ñ–æ–Ω", "–∫—Ä–æ—Å—Å", "–¥–∏—Å—Ç–∞–Ω—Ü–∏—è", "—Å–ø–æ—Ä—Ç—Å–º–µ–Ω", "–±–µ–≥–æ–≤—ã–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏"],
    "mountains": ["–≥–æ—Ä—ã", "–≥–æ—Ä–Ω—ã–π —Ç—É—Ä–∏–∑–º", "–∞–ª—å–ø–∏–Ω–∏–∑–º", "–ø–æ—Ö–æ–¥", "–≥–æ—Ä—ã –Ω–∞ –≤—ã—Å–æ—Ç–µ", "–≥–æ—Ä–Ω—ã–µ —Ç—Ä–æ–ø—ã", "—ç–∫—Å–ø–µ–¥–∏—Ü–∏—è"]
}


def extract_keywords(post_text):
    """üîë –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å —É—á—ë—Ç–æ–º —Å–∏–Ω–æ–Ω–∏–º–æ–≤ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
    log_info(f"üìù –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç: {post_text}")

    # –ü—Ä–∏–º–µ–Ω—è–µ–º spaCy –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–∞
    doc = nlp(post_text.lower())  # –ü—Ä–∏–≤–æ–¥–∏–º —Ç–µ–∫—Å—Ç –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É

    # –õ–æ–≥–∏—Ä—É–µ–º —Ç–æ–∫–µ–Ω—ã
    tokens = [token.text for token in doc]
    log_info(f"üîç –†–∞–∑–¥–µ–ª—ë–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–∞ —Ç–æ–∫–µ–Ω—ã: {tokens}")

    # –ò—â–µ–º –∫–ª—é—á–µ–≤—ã–µ —Ç–µ–º—ã
    keywords = []

    # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ —Å–∏–Ω–æ–Ω–∏–º–∞–º –∏ –∏—â–µ–º –∏—Ö –≤ —Ç–µ–∫—Å—Ç–µ
    for key, words in synonyms.items():
        for word in words:
            if word in tokens:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–º —Ç–µ–∫—Å—Ç–µ spaCy
                keywords.append(key)
                log_info(f"‚úÖ –ù–∞–π–¥–µ–Ω–∞ –∫–ª—é—á–µ–≤–∞—è —Ç–µ–º–∞: {key} (–ø–æ —Å–ª–æ–≤—É '{word}')")
                break  # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ, –ø—Ä–µ–∫—Ä–∞—â–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –¥–ª—è —ç—Ç–æ–π —Ç–µ–º—ã

    # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤, –ø—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å —Ç–µ–º—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º NLP
    if not keywords:
        extracted_topic = analyze_text(doc)
        keywords.append(extracted_topic)
        log_warning(f"‚ö†Ô∏è –¢–µ–º–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –ø–æ —Å–∏–Ω–æ–Ω–∏–º–∞–º. –ò—Å–ø–æ–ª—å–∑—É–µ–º NLP: {extracted_topic}")

    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–µ—Ä–≤—ã–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
    final_keywords = ", ".join(keywords[:3])  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–æ 3-—Ö –∫–ª—é—á–µ–≤—ã—Ö —Ç–µ–º
    log_info(f"üîë –ò—Ç–æ–≥–æ–≤—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {final_keywords}")
    return final_keywords


def analyze_text(doc):
    """üîç –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ NLP –¥–ª—è –ø–æ–∏—Å–∫–∞ –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–º—ã"""
    for token in doc:
        if token.pos_ in ["NOUN", "PROPN"]:  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ
            return token.text
    return "nature"  # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º 'nature'
